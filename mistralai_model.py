from huggingface_hub import InferenceClient

# ‡πÉ‡∏ä‡πâ Access Token ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
API_TOKEN = ""  # üîπ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Token ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
MODEL_NAME = "mistralai/Mistral-7B-Instruct-v0.2"

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Client
client = InferenceClient(model=MODEL_NAME, token=API_TOKEN)

# Prompt ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏≠‡∏ö
prompt = "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏°‡∏±‡∏Ñ‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô"

# ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô API
response = client.text_generation(prompt, max_new_tokens=200)

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
print(response)
